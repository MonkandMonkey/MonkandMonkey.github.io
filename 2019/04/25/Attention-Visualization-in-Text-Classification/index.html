<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="MonkandMonkey">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="Attention Visualization in Text Classification"/>
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="Amy"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Title -->
  
  <title>Attention Visualization in Text Classification - Amy</title>


  <link rel="shortcut icon" href="/images/amy/bath.png">

  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">

  <!----------------------------
  https://github.com/GallenHu/hexo-theme-Daily

 _____            _   _
|  __ \          (_) | |
| |  | |   __ _   _  | |  _   _
| |  | |  / _` | | | | | | | | |
| |__| | | (_| | | | | | | |_| |
|_____/   \__,_| |_| |_|  \__, |
                          __/ |
                         |___/

    ---------------------------><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body>

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="/images/amy/owl.jpg" alt="Amy" height="60">
        
      </a>
    </div>
    <!-- Navigation -->
    <nav class="navbar">
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  Home
                
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  Archive
                
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  About
                
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            Attention Visualization in Text Classification
            
          </h1>
          <p class="posted-on">
          2019-04-25
          </p>
          <div class="tags-links">
            
              
                <a href="/tags/tech/" rel="tag">
                  tech
                </a>
              
                <a href="/tags/python/" rel="tag">
                  python
                </a>
              
                <a href="/tags/nlp/" rel="tag">
                  nlp
                </a>
              
                <a href="/tags/keras/" rel="tag">
                  keras
                </a>
              
                <a href="/tags/attn/" rel="tag">
                  attn
                </a>
              
            
          </div>
        </div>
        <!-- Post Main Content -->
        <div class="entry-content">
          <p>This post will give an brief introduction of <code>attention</code> mechanism in seq2seq model and then show you how does the attention work in text classification. Finally we visualize the <code>attention weights</code> in a RNN based <code>text classification</code> model. The model is implemented in <code>Keras</code>.</p>
<h2 id="1-About-attention"><a href="#1-About-attention" class="headerlink" title="1. About attention"></a>1. About attention</h2><p>Attention first comes from the way human beings observing things. Our brains use visual attention mechanism to process the visual information we received. When observing a picture, we first have an quick overview, and then our attention focus on several particular parts to gain the information we want further.<br>Except for the computer vision application, attention mechanism is useful in many NLP tasks too. Such as <code>machine translation</code> and <code>text classification</code>. You may be familiar with the model <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">seq2seq + attention</a> which is wellknown in machine translation, which is shown below. Among which the $a(s_{t-1}, h<em>j)$ is a function for computing the correlation between decoder state $s</em>{t-1}$ and each encoder state $h_j$.<br><img src="/images/tensorflow/seq2seq-Attention.jpg" width="50%" height="50%"><br>The attention mechanism’s expression are as follows:<br><img src="/images/tensorflow/attn-function-0.jpg" width="25%" height="25%"><br><img src="/images/tensorflow/attn-function-4.jpg" width="50%" height="50%"></p>
<h2 id="2-Attention-in-text-classification"><a href="#2-Attention-in-text-classification" class="headerlink" title="2. Attention in text classification"></a>2. Attention in text classification</h2><p>For the case of text classification, since only 1 label is asked to be output, we don’t have a decoder, neither do the decoder state. So the correlation function $a(s_{t-1}, h_j)$ is changed to $a(h_j)$ for computing the correlation between the label and each encoder state $h_j$. For the details of function $a(⋅)$ and the function $a_j = max(e_j)$ which only keeps the most significant correlation value, you can design them by your need. And the attention weights represents the relationship between the output label and the input tokens.<br>$$ e_j = a(h_j) $$<br><img src="/images/tensorflow/attn-function-3.jpg" width="18%" height="18%"></p>
<h2 id="3-Visualize-attention-weights"><a href="#3-Visualize-attention-weights" class="headerlink" title="3. Visualize attention weights"></a>3. Visualize attention weights</h2><p>This section will introduce you how to visualize the <code>attention weights</code> in a text classification task.<br>In this procedure, we need to get the words and their corresponding attention wgts and the final output label. The final effect will looks like this.<br><img src="/images/tensorflow/attn-vis.jpg" width="90%" height="90%"><br>The main steps are as follows.</p>
<ol>
<li>Train a usual text classification model(mdl) and save its model wgts.(using checkpoints)</li>
<li>The attention layer (better designed as an independent class) has a switch to control output probs or output attn wgts.</li>
<li>Build 2 models and load the saved model wgts. One for predict the label(pred_mdl), the other for get the attention wgts(attn_mdl).<br>First we provide you an implementation of Attention layer with such switch.<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="string">"""</span></div><div class="line">File: attnlayer.py</div><div class="line">Date: 2019-03-30 15:46</div><div class="line">Author: amy</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> logging</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers, layers</div><div class="line"><span class="keyword">from</span> keras.engine <span class="keyword">import</span> InputSpec, Layer</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> normalize</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"></div><div class="line"><span class="keyword">import</span> config <span class="keyword">as</span> conf</div><div class="line"></div><div class="line">logger = logging.getLogger(<span class="string">"console_file"</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionWeightedAverage</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Computes a weighted average of the different channels across timesteps.</div><div class="line">    Uses 1 parameter pr. channel to compute the attention value for a single timestep.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, return_attention=False, **kwargs)</span>:</span></div><div class="line">        self.init = initializers.get(<span class="string">'glorot_uniform'</span>)</div><div class="line">        <span class="comment"># the swith to control return values</span></div><div class="line">        self.return_attention = return_attention</div><div class="line">        super(AttentionWeightedAverage, self).__init__(**kwargs)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self)</span>:</span></div><div class="line">        config = &#123;</div><div class="line">            <span class="string">'return_attention'</span>: self.return_attention,</div><div class="line">        &#125;</div><div class="line">        base_config = super(AttentionWeightedAverage, self).get_config()</div><div class="line">        <span class="keyword">return</span> dict(list(base_config.items()) + list(config.items()))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        <span class="string">''' define your attention layer params here: '''</span></div><div class="line">        self.input_spec = [InputSpec(ndim=<span class="number">3</span>)]</div><div class="line">        <span class="keyword">assert</span> len(input_shape) == <span class="number">3</span></div><div class="line">        self.W = self.add_weight(name=<span class="string">"Wa"</span>,</div><div class="line">                                 shape=(input_shape[<span class="number">-1</span>], conf.n_classes),</div><div class="line">                                 initializer=self.init,</div><div class="line">                                 trainable=<span class="keyword">True</span>)</div><div class="line">        self.trainable_weights = [self.W]</div><div class="line">        super(AttentionWeightedAverage, self).build(input_shape)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">''' define your attention function here: '''</span></div><div class="line">        ej = a(hj)  <span class="comment"># implement it by yourself.</span></div><div class="line">        <span class="comment"># find the most important value</span></div><div class="line">        <span class="comment"># ai: (n_classes, 1)</span></div><div class="line">        ai = K.max(ej, axis=<span class="number">-1</span>)</div><div class="line">        <span class="comment"># softmax to get the attention weights</span></div><div class="line">        att_weights = layers.Softmax(axis=<span class="number">-1</span>)(ai)</div><div class="line">        <span class="comment"># weight-avg x</span></div><div class="line">        weighted_inputs = x * K.expand_dims(att_weights)</div><div class="line">        <span class="comment"># output probs for each class</span></div><div class="line">        result = K.sum(weighted_inputs, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> self.return_attention:</div><div class="line">            <span class="comment"># return both attn output and attn wgts (attn scores for each class)</span></div><div class="line">            <span class="keyword">return</span> [result, att_weights]</div><div class="line">        <span class="comment"># only return attn layer output: wgted avg hidden stateds</span></div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_output_shape_for</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.compute_output_shape(input_shape)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        output_len = input_shape[<span class="number">2</span>]</div><div class="line">        <span class="keyword">if</span> self.return_attention:</div><div class="line">            <span class="keyword">return</span> [(input_shape[<span class="number">0</span>], output_len), (input_shape[<span class="number">0</span>], input_shape[<span class="number">1</span>])]</div><div class="line">        <span class="keyword">return</span> input_shape[<span class="number">0</span>], output_len</div></pre></td></tr></table></figure>
</li>
</ol>
<p>Second, we provide you a RNN model using the predefined attention layer.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">birnn_model</span><span class="params">(return_attention=False)</span>:</span></div><div class="line">    logger.info(<span class="string">"using [birnn] model..."</span>)</div><div class="line">    <span class="comment"># word embedding layer</span></div><div class="line">    w_inp = Input(shape=(p[<span class="string">'w_maxlen'</span>],), name=<span class="string">'word_input'</span>)</div><div class="line"></div><div class="line">    w_emb = Embedding(w_vocab_size, p[<span class="string">'w_embdim'</span>],</div><div class="line">                      embeddings_initializer=random_uniform(minval=<span class="number">-1.</span>, maxval=<span class="number">1.</span>),</div><div class="line">                      name=<span class="string">'word_embedding'</span>)(w_inp)</div><div class="line">    w_emb = SpatialDropout1D(p[<span class="string">'w_embdrop'</span>])(w_emb)</div><div class="line"></div><div class="line">    <span class="comment"># rnn cell type</span></div><div class="line">    rnn = LSTM</div><div class="line"></div><div class="line">    <span class="comment"># birnn</span></div><div class="line">    w_fw = rnn(p[<span class="string">'w_featdim'</span>], name=<span class="string">'word_fw_rnn'</span>, return_sequences=<span class="keyword">True</span>)(w_emb)</div><div class="line">    w_bw = rnn(p[<span class="string">'w_featdim'</span>], go_backwards=<span class="keyword">True</span>, return_sequences=<span class="keyword">True</span>, name=<span class="string">'word_bw_rnn'</span>)(w_emb)</div><div class="line">    w_feat = concatenate([w_fw, w_bw])</div><div class="line"></div><div class="line">    <span class="comment"># attention</span></div><div class="line">    ha = AttentionWeightedAverage(name=<span class="string">"attn"</span>, return_attention=return_attention)(w_feat, pre_embs=emo_embs)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> return_attention <span class="keyword">is</span> <span class="keyword">True</span>:</div><div class="line">        m = Model(inputs=[w_inp], outputs=ha)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        h = concatenate([c_feat, ha])</div><div class="line">        <span class="comment"># output</span></div><div class="line">        emo = Dense(conf.n_classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'emoji'</span>)(h)</div><div class="line"></div><div class="line">        m = Model(inputs=[w_inp], outputs=[emo])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> m</div></pre></td></tr></table></figure></p>
<p>Finally, we will show you how to gain and visualize attn wgts.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Visualizer</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></div><div class="line">                 word_info_file=<span class="string">"data/word_info.txt"</span>,</div><div class="line">                 sample_file=<span class="string">"data/test.txt"</span>):</div><div class="line">        <span class="string">"""</span></div><div class="line">        Visualizes attention weights.</div><div class="line">        Args:</div><div class="line">            word_vocab_file: idx2word dict file.</div><div class="line">            sample_file: input data file to visualize.</div><div class="line">        """</div><div class="line">        self.w_inps, self.labels =  <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">        self.word2idx, self.w_maxlen = <span class="keyword">None</span>, <span class="keyword">None</span></div><div class="line">        self.idx2word = <span class="keyword">None</span></div><div class="line">        self.w_vocab_size = <span class="keyword">None</span></div><div class="line"></div><div class="line">        self.word_lists = []</div><div class="line">        self.texts = <span class="keyword">None</span></div><div class="line"></div><div class="line">        self.pred_model = <span class="keyword">None</span></div><div class="line">        self.attn_model = <span class="keyword">None</span></div><div class="line">        self.weights_file = <span class="string">"checkpoints/attn_vis.epo_10.hdf5"</span></div><div class="line"></div><div class="line">        self.n_sample = <span class="keyword">None</span></div><div class="line"></div><div class="line">        self.correct_file = <span class="string">"./attention_maps/correct.txt"</span></div><div class="line">        self.wrong_file = <span class="string">"./attention_maps/wrong.txt"</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_models</span><span class="params">(self, pred_model, attn_model)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Sets the models to use.</div><div class="line"></div><div class="line">        Args:</div><div class="line">            pred_model: the prediction model</div><div class="line">            attn_model: the model that outputs the activation maps</div><div class="line"></div><div class="line">        Returns:</div><div class="line"></div><div class="line">        """</div><div class="line">        self.pred_model = pred_model</div><div class="line">        self.attn_model = attn_model</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention_map</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">            Text to visualze attention map for.</div><div class="line">        """</div><div class="line">        <span class="comment"># predict labels</span></div><div class="line">        logger.info(<span class="string">"start attention mapping..."</span>)</div><div class="line">        y_preds_onehot = self.pred_model.predict(x=&#123;<span class="string">'word_input'</span>: self.w_inps&#125;, batch_size=<span class="number">128</span>)</div><div class="line">        y_preds = np.argmax(y_preds_onehot, axis=<span class="number">-1</span>)</div><div class="line">        y_trues = self.labels</div><div class="line"></div><div class="line">        <span class="comment"># get the activation map</span></div><div class="line">        ret = self.attn_model.predict(x=&#123; <span class="string">'word_input'</span>: self.w_inps&#125;, batch_size=<span class="number">128</span>)</div><div class="line">        attn_wgts = ret[<span class="number">1</span>]</div><div class="line"></div><div class="line">        <span class="comment"># save to correct_file and wrong_file</span></div><div class="line">        logger.info(<span class="string">"saving to files ..."</span>)</div><div class="line">        <span class="keyword">with</span> open(self.correct_file, <span class="string">"w"</span>, encoding=conf.enc) <span class="keyword">as</span> c_fw, \</div><div class="line">                open(self.wrong_file, <span class="string">"w"</span>, encoding=conf.enc) <span class="keyword">as</span> w_fw:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_sample):</div><div class="line">                wl = self.word_lists[i]</div><div class="line">                text = self.texts[i]</div><div class="line">                y_pred = y_preds[i]</div><div class="line">                y_true = y_trues[i]</div><div class="line">                pred = y_pred</div><div class="line">                label = y_true</div><div class="line">                wgts = attn_wgts[i]</div><div class="line">                <span class="keyword">assert</span> len(wl) == len(wgts)</div><div class="line">                ss = <span class="string">""</span></div><div class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(len(wl)):</div><div class="line">                    w, wgt = wl[k], wgts[k]</div><div class="line">                    <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"PAD"</span>, ] <span class="comment"># ignore special tokens:</span></div><div class="line">                        ss += <span class="string">"(&#123;&#125;, &#123;:.4f&#125;) "</span>.format(w, wgt)</div><div class="line"></div><div class="line">                <span class="keyword">if</span> y_pred == y_true:</div><div class="line">                    c_fw.write(<span class="string">"[&#123;&#125;]: &#123;&#125;\n"</span>.format(i, text))</div><div class="line">                    c_fw.write(<span class="string">"true: &#123;&#125;, pred: &#123;&#125; || &#123;&#125;\n"</span>.format(label, pred, ss))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    w_fw.write(<span class="string">"[&#123;&#125;]: &#123;&#125;\n"</span>.format(i, text))</div><div class="line">                    w_fw.write(<span class="string">"[]true: &#123;&#125;, pred: &#123;&#125; || &#123;&#125;\n"</span>.format(label, pred, ss))</div><div class="line">        logger.info(<span class="string">"finish attention mapping..."</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_visualize</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># preprocess data</span></div><div class="line">        w_tst, w_vocab, tst_labels, tst_docs, word2idx = load_data(test_file)</div><div class="line">        self.w_inps = w_tst</div><div class="line">        self.labels = np.argmax(tst_labels, axis=<span class="number">-1</span>)</div><div class="line">        self.word2idx = word2idx</div><div class="line">        self.idx2word = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.word2idx.items()&#125;</div><div class="line">        self.w_vocab_size = len(self.word2idx)</div><div class="line">        <span class="comment"># map idxes to word lists</span></div><div class="line">        <span class="keyword">for</span> seq <span class="keyword">in</span> self.w_inps:</div><div class="line">            wl = []</div><div class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> seq:</div><div class="line">                <span class="keyword">if</span> idx <span class="keyword">in</span> self.idx2word:</div><div class="line">                    wl.append(self.idx2word[idx])</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    wl.append(<span class="string">"OOV"</span>)</div><div class="line">            self.word_lists.append(wl)</div><div class="line"></div><div class="line">        self.texts = tst_docs</div><div class="line">        self.n_sample = len(self.texts)</div><div class="line"></div><div class="line">        <span class="comment"># build 2 models and load weights</span></div><div class="line">        pred_model = birnn_model(return_attention=<span class="keyword">False</span>)</div><div class="line">        build_model(pred_model)</div><div class="line">        pred_model.load_weights(<span class="string">'checkpoints/attn_vis.epo_10.hdf5'</span>, by_name=<span class="keyword">True</span>)</div><div class="line">        pred_model.summary()</div><div class="line"></div><div class="line">        attn_model = birnn_model(return_attention=<span class="keyword">True</span>)</div><div class="line">        build_model(attn_model)</div><div class="line">        attn_model.load_weights(<span class="string">'checkpoints/attn_vis.epo_10.hdf5'</span>, by_name=<span class="keyword">True</span>)</div><div class="line">        attn_model.summary()</div><div class="line"></div><div class="line">        self.set_models(pred_model, attn_model)</div><div class="line"></div><div class="line">        <span class="comment"># visualize</span></div><div class="line">        self.attention_map()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    vis = Visualizer()</div><div class="line">    vis.do_visualize()</div></pre></td></tr></table></figure></p>
<p>By the way, after getting the words, prediction label and the attention weights, you can draw the above picture in differenent ways. The larger the attention weight is, the darker the text background color should be.</p>

        </div>
      </article>
    </div>
    <!-- Comments -->
    <div class="container">
      



    </div>
    <!-- Pre or Next -->
    <div class="nav-links">
      
      
        <div class="nav-next">
          <a href="/2019/02/19/Tensorflow-template/" rel="prev">Newer Posts <span class="meta-arraw meta-arraw-right"></span></a>
        </div>
      
    </div>

  </div>
</div>


  <!-- Footer -->
  <!-- Footer-widgets -->
<div class="footer-widgets">
  <div class="row inside-wrapper">
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Me</h1>
        <div class="custom-widget-content">
          
          <ul><li>MonkandMonkey</li></ul>
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Contact</h1>
        <div class="widget-text">
          
            
              <a href="https://github.com/MonkandMonkey" class="icon icon-github" target="_blank">github</a>
            
              <a href="mailto:chenjing.amy@pku.edu.cn" class="icon icon-mail" target="_blank">mail</a>
            
          
        </div>
      </aside>
    </div>
    
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Search</h1>
        <div class="widget-text">
          <form onSubmit="return appDaily.submitSearch('')">
            <p>
              <input type="text" placeholder="search..." id="homeSearchInput">
            </p>
             <!--<input type="submit" value="GO">-->
          </form>
        </div>
      </aside>
    </div> 
  </div>
</div>
<!-- Footer -->
<footer class="site-info">
  <p>
    <span>Amy &copy; 2019</span>
    
      <span class="split">|</span>
      <span>Powered by MonkandMonkey</a></span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>

<script>
  var disqus_shortname = 'monkandmonkey3';

  
  var disqus_url = 'http://localhost:4000/2019/04/25/Attention-Visualization-in-Text-Classification/';
  

  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





</body>

</html>